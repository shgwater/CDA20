---
title: 2018-3-28决策树实例
tags: 决策树,R实例
grammar_cjkRuby: true
---
# 决策树实例
## 1 项目背景
有一款医疗险产品主要面向
## 2 决策树基本介绍
### 有监督算法和无监督算法
**有监督算法：最常见为回归&分类问题。**
- 从给定的训练数据中学习一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集包括输入和输出，也可以说是特征和目标。训练集中的目标是人为标注的。
- 利用历史结果回答商业问题

**无监督算法：最常见为密度估计&聚类问题**
- 与监督学习相比，训练集没有认为标注的结果
- AlphaGo
- 最终不知道目标变量是什么。

### 决策树-Example（例子）
- 目标：通过是否拥有房产、婚姻状况、收入来预测某个群体是否具有偿债能力。
- 自变量：是否拥有房产、是否结婚、收入
- 因变量：是否有偿债能力。

### 分类问题
在数据挖掘领域，分类是利用一个分类器（分类模型），将数据映射到预先定义好的群组或类。
一般的分类问题可分为两分类和多分类两种。
- 完整的分类过程包括三个步骤：学习模块，验证模块，测试模块
### 多分类问题如何转化
假设我们的目标变量有五种分类，如何转化？
- 一对多
将类别1看作证类，其余2,3,4,5看作负累（一对多），这样拿一个样本来，可以告诉你是不是属于类别一，如果不属于，再在1,2,3,4中继续寻找。
- 一对一
从1-5中任意选取2种类别来分类，得到5x4/2=10中分类器，每一个分类器只告诉你是第一类还是第二类，或者是第一类还是第三类，统计所有分类器的票数，根据票数得到分类结果。
### 分类问题VS回归问题
- 分类问题：尝试从一个或多个连续、分类预测变量中预测一个分类变量
- 回归问题：尝试从一个或多个连续、分类预测变量中预测一个**连续变量**
- 分类和回归的区别主要在于输出变量的类型
  - 定量输出称为回归，或者说是连续变量的预测。
  - 定性输出称为分类，或者说是离散变量的预测。
### 分类模型的性能
分类模型的性能评价指标
- 混淆矩阵（Confusion Matrix)

| | |测试情况| |
| | | 正例| 反例 |
|   ---  |  --- |  ---   |  ---   |
|**真实情况**|    **正例** |   TP（真正例）  |   FN（假反例  |
| |    **反例** |    EP（假正例） |   TN（真反例）  |

- 错误率（Error Rate）
- 敏感性( Sensitivity, True Positive Rate） = 真正例/实际正例总数
- 特异性(Specificity, True Negative Rate) = 真反例/实际反例总数
- ROC曲线： x轴为1-specificity， y轴为sensitivity
- Lift曲线： x轴为depth=(TP+FP)/(TP+FP+FN+TN）预测成正例的比例， y轴为lift=（TP/(TP+FP)）/((TP+FN)/(TP+FP+FN+TN))，预测正例的比例比上实际正例的比例。

### ROC曲线VSLift曲线
ROC曲线越凸越好。

Lift曲线的阈值从左到右，y变得越来越小，这样的说明模型效果好。

### 决策树基本介绍
- 决策树是什么？
  - 以事例为基础的归纳学习算法，着眼于从一组无次序、无规则的事例中推导出决策树表现形式的分类规则
- 决策树能做什么？
  - 分类器,可以对未知数据进行分类
  - 预测模型，可以对未知数据进行预测（不常用）
- 根据输出变量的类型不同有分类树和回归树：
  - 回归树，输出为连续型变量；
  - 分类树，输出是分类标签或离散变量。

#### 决策树创建过程
- 选择合理的划分节点
	- 分裂准则
	- 分类树：信息增益，信息增益率，基尼系数
	- 回归树：最小方差
- 判断划分停止点
 - 剪枝：预剪枝、后剪枝。
- 叶节点的输出
  - 分类树：取叶节点中数量变量占比最多的那一类。
  - 回归树：结合业务需求取较大或较小响应变量的均值对应的分类。

#### 决策树优点缺点
优点
1. 分类规则清洗，结果容易理解
2. 计算量相对较小，实现速度快。
3. 非线性，非参数方法。
4. 不需要预选变量，可处理异常值（*单独一类*）、缺失值、不同量纲值。
  
缺点
1. 相比逻辑回归模型，准确性略有不足，常用来作为预先选变量的方法。
2. 容易过拟合（*在训练集上准确度很高，在测试集上准确率迅速下降，可能出现了过拟合*）。

## 3 决策树原理
### ID3算法
基础信息 **熵 entropy：** 

假设事件a的信息量为 `!$I(a_i)$`
则`!$I(a_i)=p(a_i)log_2  \frac 1 p(a_i)$`；`!$p(a_i)$`表示事件a发生的概率。

假设有n个时间 `!$a_1,a_2,a_3...a_n $`则
`!$I(a_1,a_2,...,a_n)= \sum_{i=1}^nP(a_i)log_2 \frac 1 {P(a_i)}=-\sum_{i=1}^nP(a_i)log_2P(a_i) $`
规定：当`!$P(a_i)=0$`时，`!$I(a_i)=0$`

当数据越纯净时，熵越小，即纯净度越高。熵越大，即混乱度越高。
使用A分割S

`!$entropy(S,A)=\sum_{i=1}^m\frac {|S_i|} {|S|}entropy(S_i)$`

`!$entropy(S,A)$`越高，混乱度越高。

先用A划分后，**信息增益**为

`!$gain(S,A)=entropy(S)-entropy(S,A)$`

``` r
a<-cadsfasdfasdfasdfasdfasdfasdfasdfasdfasdfffffffffffddddddddddddddddddddddddddddddddddddddddddddddddddddd
```


----------


----------
